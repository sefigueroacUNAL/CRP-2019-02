{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracción de características\n",
    "Las técnicas que hemos visto hasta ahora se usan generalmente cuando no tenemos una gran cantidad de variables en nuestro conjunto de datos. Estas son más o menos técnicas de selección de características. En las próximas secciones, trabajaremos con el conjunto de datos Fashion MNIST, que consiste en imágenes que pertenecen a diferentes tipos de ropa, por ejemplo, camisetas, pantalones, bolsos, etc. El conjunto de datos se puede descargar desde la práctica \" IDENTIFICAR LA ROPA \" problema.\n",
    "\n",
    "El conjunto de datos tiene un total de 70,000 imágenes, de las cuales 60,000 están en el conjunto de entrenamiento y las 10,000 restantes son imágenes de prueba. Para el alcance de este artículo, trabajaremos solo en las imágenes de capacitación. El archivo del tren está en formato zip. Una vez que extraiga el archivo zip, obtendrá un archivo .csv y una carpeta de tren que incluye estas 60,000 imágenes. La etiqueta correspondiente de cada imagen se puede encontrar en el archivo 'train.csv'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Análisis Factorial\n",
    "Supongamos que tenemos dos variables: Ingresos y Educación. Estas variables potencialmente tendrán una alta correlación ya que las personas con un nivel de educación superior tienden a tener ingresos significativamente más altos, y viceversa.\n",
    "\n",
    "En la técnica de Análisis Factorial, las variables se agrupan por sus correlaciones, es decir, todas las variables en un grupo particular tendrán una alta correlación entre ellas, pero una baja correlación con las variables de otros grupos. Aquí, cada grupo se conoce como un factor. Estos factores son pequeños en comparación con las dimensiones originales de los datos. Sin embargo, estos factores son difíciles de observar.\n",
    "\n",
    "Primero leamos todas las imágenes contenidas en la carpeta de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import cv2\n",
    "images = [cv2.imread(file) for file in glob('train/*.png')]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Ahora convertiremos estas imágenes en un  formato de matriz numpy para que podamos realizar operaciones matemáticas y también trazar las imágenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = np.array(images)\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como puede ver arriba, es una matriz tridimensional. Debemos convertirlo a 1 dimensión, ya que todas las próximas técnicas solo toman una entrada unidimensional. Para hacer esto, necesitamos aplanar las imágenes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-12f08543130b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m60000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "image = []\n",
    "for i in range(0,60000):\n",
    "    img = images[i].flatten()\n",
    "    image.append(img)\n",
    "image = np.array(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora creemos un dataframe que contenga los valores de píxeles de cada píxel individual presente en cada imagen, y también sus etiquetas correspondientes (para las etiquetas, haremos uso del archivo train.csv )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")     # Give the complete path of your train.csv file\n",
    "feat_cols = [ 'pixel'+str(i) for i in range(image.shape[1]) ]\n",
    "df = pd.DataFrame(image,columns=feat_cols)\n",
    "df['label'] = train['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora descompondremos el conjunto de datos usando Análisis Factorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import FactorAnalysis\n",
    "FA = FactorAnalysis (n_components = 3) .fit_transform (df [feat_cols] .values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_components define el número de factores en los datos transformados. Después de transformar los datos, es hora de visualizar los resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.title('Factor Analysis Components')\n",
    "plt.scatter(FA[:,0], FA[:,1])\n",
    "plt.scatter(FA[:,1], FA[:,2])\n",
    "plt.scatter(FA[:,2],FA[:,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver todos los diferentes factores en el gráfico anterior. Aquí, el eje xy el eje y representan los valores de los factores descompuestos. Como se mencionó anteriormente, es difícil observar estos factores individualmente, pero hemos podido reducir con éxito las dimensiones de nuestros datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Análisis de Componente Principal (PCA)\n",
    "PCA es una técnica que ayuda a extraer un nuevo conjunto de variables de un gran conjunto de variables existente. (Feature Extraction) Estas variables recién extraídas se denominan Componentes principales. Para su referencia rápida, a continuación se detallan algunos de los puntos clave que debe conocer sobre PCA antes de continuar:\n",
    "\n",
    "- Un componente principal es una combinación lineal de las variables originales.\n",
    "- Los componentes principales se extraen de tal manera que el primer componente principal explica la varianza máxima en el conjunto de datos\n",
    "- El segundo componente principal intenta explicar la varianza restante en el conjunto de datos y no está correlacionado con el primer componente principal\n",
    "- El tercer componente principal intenta explicar la varianza que no se explica por los dos primeros componentes principales, etc.\n",
    "\n",
    "Antes de continuar, trazaremos al azar algunas de las imágenes de nuestro conjunto de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rndperm = np.random.permutation(df.shape[0])\n",
    "plt.gray()\n",
    "fig = plt.figure(figsize=(20,8))\n",
    "for i in range(0,15):\n",
    "    ax = fig.add_subplot(3,5,i+1)\n",
    "    ax.matshow(df.loc[rndperm[i],feat_cols].values.reshape((28,28*3)).astype(float))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementemos PCA usando Python y transformemos el conjunto de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=4)\n",
    "pca_result = pca.fit_transform(df[feat_cols].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, n_components decidirá el número de componentes principales en los datos transformados. Visualicemos cuánta varianza se puede explicar  usando estos 4 componentes. Usaremos explained_variance_ratio_ para calcular lo mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(4), pca.explained_variance_ratio_)\n",
    "plt.plot(range(4), np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.title(\"Component-wise and Cumulative Explained Variance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el gráfico anterior, la línea azul representa la varianza explicada por componentes, mientras que la línea naranja representa la varianza explicativa acumulativa. Podemos explicar alrededor del 60% de varianza en el conjunto de datos utilizando solo cuatro componentes. Ahora tratemos de visualizar cada uno de estos componentes descompuestos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.style.use('fivethirtyeight')\n",
    "fig, axarr = plt.subplots(2, 2, figsize=(12, 8))\n",
    "sns.heatmap(pca.components_[0, :].reshape(28, 84), ax=axarr[0][0], cmap='gray_r')\n",
    "sns.heatmap(pca.components_[1, :].reshape(28, 84), ax=axarr[0][1], cmap='gray_r')\n",
    "sns.heatmap(pca.components_[2, :].reshape(28, 84), ax=axarr[1][0], cmap='gray_r')\n",
    "sns.heatmap(pca.components_[3, :].reshape(28, 84), ax=axarr[1][1], cmap='gray_r')\n",
    "axarr[0][0].set_title(\n",
    "\"{0:.2f}% Explained Variance\".format(pca.explained_variance_ratio_[0]*100),\n",
    "fontsize=12\n",
    ")\n",
    "axarr[0][1].set_title(\n",
    "\"{0:.2f}% Explained Variance\".format(pca.explained_variance_ratio_[1]*100),\n",
    "fontsize=12\n",
    ")\n",
    "axarr[1][0].set_title(\n",
    "\"{0:.2f}% Explained Variance\".format(pca.explained_variance_ratio_[2]*100),\n",
    "fontsize=12\n",
    ")\n",
    "axarr[1][1].set_title(\n",
    "\"{0:.2f}% Explained Variance\".format(pca.explained_variance_ratio_[3]*100),\n",
    "fontsize=12\n",
    ")\n",
    "axarr[0][0].set_aspect('equal')\n",
    "axarr[0][1].set_aspect('equal')\n",
    "axarr[1][0].set_aspect('equal')\n",
    "axarr[1][1].set_aspect('equal')\n",
    "\n",
    "plt.suptitle('4-Component PCA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada dimensión adicional que agreguemos a la técnica PCA captura cada vez menos la varianza en el modelo. El primer componente es el más importante, seguido del segundo, luego el tercero, y así sucesivamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Descomposición Singular de Valores  (SVD)\n",
    "También podemos usar la Descomposición de valor singular  (SVD) para descomponer nuestro conjunto de datos original en sus componentes, lo que resulta en una reducción de la dimensionalidad. \n",
    "\n",
    "SVD descompone las variables originales en tres matrices constituyentes. Se utiliza esencialmente para eliminar características redundantes del conjunto de datos. Utiliza el concepto de valores propios (eigen values) y vectores propios (eigen vectors) para determinar esas tres matrices. SVD puede ser aplicado para reducir las dimensiones en nuestro conjunto de datos.\n",
    "\n",
    "Implementemos SVD y descompongamos nuestras variables originales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD \n",
    "svd = TruncatedSVD(n_components=3, random_state=42).fit_transform(df[feat_cols].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualicemos las variables transformadas trazando los dos primeros componentes principales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.title('SVD Components')\n",
    "plt.scatter(svd[:,0], svd[:,1])\n",
    "plt.scatter(svd[:,1], svd[:,2])\n",
    "plt.scatter(svd[:,2],svd[:,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Análisis de Componente Independiente  (ICA)\n",
    "El análisis de componentes independientes (ICA) se basa en la teoría de la información y también es una de las técnicas de reducción de dimensionalidad más utilizadas. La principal diferencia entre PCA e ICA es que PCA busca factores no correlacionados mientras que ICA busca factores independientes.\n",
    "\n",
    "Si dos variables no están correlacionadas, significa que no hay una relación lineal entre ellas. Si son independientes, significa que no dependen de otras variables. Por ejemplo, la edad de una persona es independiente de lo que come, o de la cantidad de televisión que ve.\n",
    "\n",
    "Este algoritmo supone que las variables dadas son mezclas lineales de algunas variables latentes desconocidas. También supone que estas variables latentes son mutuamente independientes , es decir, no dependen de otras variables y, por lo tanto, se denominan componentes independientes de los datos observados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparemos visualmente PCA e ICA para obtener una mejor comprensión de cómo son diferentes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='images/pcavsica.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la imagen anterior,la imagen (a) representa los resultados de PCA, mientras que la imagen (b) representa los resultados de ICA en el mismo conjunto de datos.\n",
    "\n",
    "La ecuación de PCA es x = Wχ.\n",
    "\n",
    "Aquí,\n",
    "\n",
    "- x son las observaciones\n",
    "- W es la matriz de mezcla\n",
    "- χ es la fuente o los componentes independientes\n",
    "\n",
    "Ahora tenemos que encontrar una matriz sin mezcla para que los componentes se vuelvan lo más independientes posible. El método más común para medir la independencia de los componentes es el no gaussiano:\n",
    "\n",
    "- Según el teorema del límite central, la distribución de la suma de componentes independientes tiende a estar normalmente distribuida (gaussiana).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='images/gaussian.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Entonces podemos buscar las transformaciones que maximizan la curtosis de cada componente de los componentes independientes. La curtosis es el momento de tercer orden de la distribución. Para aprender más sobre la curtosis, dirígete  aquí .\n",
    "- Maximizar la curtosis hará que la distribución no sea gaussiana y, por lo tanto, obtendremos componentes independientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='images/non-gaussian.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La distribución anterior no es gaussiana, lo que a su vez hace que los componentes sean independientes. Intentemos implementar ICA en Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import FastICA \n",
    "ICA = FastICA(n_components=3, random_state=12) \n",
    "X=ICA.fit_transform(df[feat_cols].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí, n_components decidirá el número de componentes en los datos transformados. Hemos transformado los datos en 3 componentes usando ICA. Visualicemos qué tan bien ha transformado los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.title('ICA Components')\n",
    "plt.scatter(X[:,0], X[:,1])\n",
    "plt.scatter(X[:,1], X[:,2])\n",
    "plt.scatter(X[:,2], X[:,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos se han separado en diferentes componentes independientes que se pueden ver muy claramente en la imagen de arriba. El eje X y el eje Y representan el valor de los componentes independientes descompuestos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Métodos basados en proyecciones \n",
    "Para comenzar, necesitamos entender qué es la proyección. Supongamos que tenemos dos vectores, el vector a y el vector b , como se muestra a continuación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='images/vectors1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queremos encontrar la proyección de a en b . Deje que el ángulo entre ayb sea ∅. La proyección ( a1 ) se verá así:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='images/vectors2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a1 es el vector paralelo a b. Entonces, podemos obtener la proyección del vector a sobre el vector b usando la siguiente ecuación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='images/eq1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta ecuación tenemos:\n",
    "- a1 = proyección de a sobre b\n",
    "- b̂ = vector unitario en la dirección de b\n",
    "\n",
    "Al proyectar un vector sobre el otro, se puede reducir la dimensionalidad.\n",
    "\n",
    "En las técnicas de proyección, los datos multidimensionales se representan proyectando sus puntos en un espacio de dimensiones inferiores. Ahora discutiremos diferentes métodos de proyecciones:\n",
    "\n",
    "- Proyección en direcciones interesantes:\n",
    "Las direcciones interesantes dependen de problemas específicos pero, en general, las direcciones en las que los valores proyectados no son gaussianos se consideran interesantes.\n",
    "Similar a ICA (Análisis de Componente Independiente), la proyección busca direcciones que maximicen la curtosis de los valores proyectados como una medida de no gaussianismo.\n",
    "\n",
    "- Proyecciones sobre Variedad (Manifold)\n",
    "Hace algún tiempo se pensaba que la Tierra era plana. No importa hacia donde nos dirigieramos en la Tierra, seguía pareciendo plana (ignoremos las montañas por un tiempo). Pero si sigue caminando en una dirección, terminaremos donde comenzamos. Eso no sucedería si la Tierra fuera plana. La Tierra solo se ve plana porque somos minúsculos en comparación con el tamaño de la misma.\n",
    "\n",
    "Estas pequeñas porciones donde la Tierra se ve plana son Variedades (Manifolds), y si combinamos todos estos Manifolds obtenemos una vista a gran escala de la Tierra, es decir, los datos originales. De manera similar para una curva n-dimensional, las pequeñas piezas planas son Manifolds y una combinación de estos Manifolds nos dará la curva n-dimensional original. Veamos los pasos para la proyección en múltiples:\n",
    "- Primero buscamos un Manifold que esté cerca de los datos\n",
    "- Luego se proyectan los datos en ese manifold\n",
    "- Finalmente para la representación, desplegamos el Manifold. Hay varias técnicas para obtener la variedad, y todas estas técnicas consisten en un enfoque de tres pasos:\n",
    "- - Recopilar información de cada punto de datos para construir un gráfico que tenga puntos de datos como vértices\n",
    "- - Transformar el gráfico generado anteriormente en una entrada adecuada para los pasos de incrustación\n",
    "- - Calcular una ecuación propia (nXn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprendamos la técnica de proyección de un Manifold con un ejemplo.\n",
    "\n",
    "Si un Manifold es continuamente diferenciable a cualquier orden, se conoce como Manifold suave o diferenciable. ISOMAP es un algoritmo que tiene como objetivo recuperar la representación completa de baja dimensión de un Manifold no lineal. Se supone que el Manifold es suave.\n",
    "\n",
    "También supone que para cualquier par de puntos en el Manifold, la distancia geodésica (la distancia más corta entre dos puntos en una superficie curva) entre los dos puntos es igual a la distancia euclidiana (la distancia más corta entre dos puntos en una línea recta). Primero visualicemos la distancia geodésica y euclidiana entre un par de puntos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='images/distances.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la gráfica\n",
    "- Dn1n2 = distancia geodésica entre X1 y X2\n",
    "- dn1n2 = Distancia euclidiana entre X1 y X2\n",
    "\n",
    "ISOMAP supone que ambas distancias son iguales. Veamos ahora una explicación más detallada de esta técnica. Como se mencionó anteriormente, todas estas técnicas funcionan en un enfoque de tres pasos. Examinaremos cada uno de estos pasos en detalle:\n",
    "\n",
    "- Gráfico de vecindario\n",
    "- - El primer paso es calcular la distancia entre todos los pares de puntos de datos:\n",
    "dij = dχ (xi, xj) = || xi-xj || χ\n",
    "Donde,\n",
    "dχ (xi, xj) = distancia geodésica entre xi y xj\n",
    "|| xi-xj || = Distancia euclidiana entre xi y xj\n",
    "- - Después de calcular la distancia, determinamos qué puntos de datos son vecinos de Manifold\n",
    "- - Finalmente, se genera el gráfico de vecindad: G = G (V, ℰ), donde el conjunto de vértices V = {x1, x2, ..., xn} son puntos de datos de entrada y el conjunto de aristas ℰ = {eij} indica la relación de vecindad entre los puntos\n",
    "\n",
    "- Calcular distancias del gráfico:\n",
    "- - Ahora calculamos la distancia geodésica entre pares de puntos en el Manifold por distancias gráficas\n",
    "- - La distancia del gráfico es la distancia de ruta más corta entre todos los pares de puntos en el gráfico G\n",
    "\n",
    "- Incrustación:\n",
    "- - Una vez que tenemos las distancias, formamos una matriz simétrica (nXn) de distancia de gráfico al cuadrado\n",
    "- - Ahora elegimos vectores incorporados para minimizar la diferencia entre la distancia geodésica y la distancia del gráfico\n",
    "- - Finalmente, el gráfico G está incrustado en Y por la matriz (t Xn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementémoslo en Python y obtengamos una imagen más clara de lo que se está hablando. Realizaremos una reducción de dimensionalidad no lineal a través del mapeo isométrico. Para la visualización, solo tomaremos un subconjunto de nuestro conjunto de datos, ya que ejecutarlo en todo el conjunto de datos demanda mucho tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import manifold \n",
    "trans_data = manifold.Isomap(n_neighbors=5, n_components=3, n_jobs=-1).fit_transform(df[feat_cols][:6000].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parámetros utilizados:\n",
    "\n",
    "- n_neighbours decide el número de vecinos para cada punto\n",
    "- n_components decide el número de coordenadas para Manifold\n",
    "- n_jobs  = -1 usará todos los núcleos de CPU disponibles\n",
    "\n",
    "Visualizando los datos transformados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.title('Decomposition using ISOMAP')\n",
    "plt.scatter(trans_data[:,0], trans_data[:,1])\n",
    "plt.scatter(trans_data[:,1], trans_data[:,2])\n",
    "plt.scatter(trans_data[:,2], trans_data[:,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
